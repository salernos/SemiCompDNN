{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "aa8176a9-b74c-4541-8c73-5acf65e0ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2cc5d7-b93d-40d0-a54f-1b0482ca5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FILE = './BLCS_Data_Updated_February2023.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55b342f-41fc-469d-b686-a7bc1bd75b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLCS = pd.read_excel(LOCAL_FILE, engine='openpyxl')\n",
    "extrcdate = pd.to_datetime('2023-02-28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48cf057-9973-4954-95e2-4d2295adc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_between(a, b):\n",
    "    return (b - a).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a39180-6e61-4ea2-ac16-7e47f35e5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [c for c in BLCS.columns if 'date' in c.lower()]\n",
    "for col in date_cols:\n",
    "    BLCS[col] = pd.to_datetime(BLCS[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ac4f62-9258-4b11-892f-4ce6dcfc2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLCS = BLCS.dropna(subset=['dxdate'])\n",
    "BLCS = BLCS[BLCS['dxdate'].apply(lambda d: days_between(d, extrcdate) > 180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3aa6452-cd15-4f15-a2bb-d6045c7f7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = {'0', 'SCLC-ES', 'SCLC-LS'}\n",
    "BLCS = BLCS[~BLCS['cstage'].isin(exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a767ce88-a3d2-46b3-9a8b-017931bfba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdate_pevent(df):\n",
    "    pdate = pd.Series(pd.NaT, index=df.index)\n",
    "    pevent = pd.Series(np.nan, index=df.index)\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.notna(row.get('progressiondate')):\n",
    "            pdate.at[i] = row['progressiondate']; pevent.at[i] = row.get('progression', np.nan)\n",
    "        elif pd.notna(row.get('relapsedate')):\n",
    "            pdate.at[i] = row['relapsedate']; pevent.at[i] = row.get('relapse', np.nan)\n",
    "        elif pd.notna(row.get('progdate')) and row['progdate'] > row['dxdate']:\n",
    "            pdate.at[i] = row['progdate']; pevent.at[i] = row.get('prog', np.nan)\n",
    "    # Recode pevent: 0 or 2 -> 0; 1 -> 1\n",
    "    pevent = pevent.replace({2:0}).fillna(np.nan)\n",
    "    pevent = pevent.where(pevent==1, 0)\n",
    "    return pdate, pevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca41fc59-89b6-4ecd-8238-4c33e845b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLCS['pdate'], BLCS['pevent'] = compute_pdate_pevent(BLCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd622b7c-300d-4407-8dc7-f7e66c92b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_censor(df):\n",
    "    censordate = pd.Series(index=df.index, dtype='datetime64[ns]')\n",
    "    censorcheck = pd.Series(index=df.index, dtype='object')\n",
    "    for i, row in df.iterrows():\n",
    "        alive = row.get('alivedate')\n",
    "        death = row.get('deathdate')\n",
    "        pdate = row.get('pdate')\n",
    "        if pd.notna(alive) and alive <= row['dxdate'] and pd.notna(alive) and not (pd.isna(pdate) and pd.isna(death)):\n",
    "            censordate.at[i] = extrcdate; censorcheck.at[i] = 'Corrected Negative Survival'\n",
    "        elif pd.notna(alive) and pd.notna(death) and alive <= death:\n",
    "            censordate.at[i] = extrcdate; censorcheck.at[i] = 'Corrected Censoring with Death'\n",
    "        else:\n",
    "            censordate.at[i] = min([d for d in [alive, extrcdate, death] if pd.notna(d)])\n",
    "            censorcheck.at[i] = 'Not Corrected'\n",
    "    return censordate, censorcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e35f084-264a-43e3-b8f6-efddbbc4ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLCS['censordate'], BLCS['censorcheck'] = compute_censor(BLCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6329b94c-e57c-4d87-8a6d-a1c6d6535074",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_per_year = 365.25 * 24 * 3600\n",
    "BLCS['Ti1'] = (BLCS['pdate'] - BLCS['dxdate']).dt.total_seconds() / sec_per_year\n",
    "BLCS['Ti2'] = (BLCS['deathdate'] - BLCS['dxdate']).dt.total_seconds() / sec_per_year\n",
    "BLCS['Ci'] = (BLCS['censordate'] - BLCS['dxdate']).dt.total_seconds() / sec_per_year\n",
    "BLCS['ptime'] = np.fmin(BLCS['Ti1'], BLCS['Ci'])\n",
    "BLCS['dtime'] = np.fmin(BLCS['Ti2'], BLCS['Ci'])\n",
    "BLCS['pevent'] = np.where(BLCS['Ti1'] <= BLCS['Ci'], 1, 0)\n",
    "BLCS['devent'] = np.where(BLCS['Ti2'] <= BLCS['Ci'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf13a1d0-5751-4664-816a-69a440ca41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = BLCS\n",
    "BLCS['Yi1'] = df[['Ti1','Ti2','Ci']].min(axis=1)\n",
    "BLCS['Yi2'] = df[['Ti2','Ci']].min(axis=1)\n",
    "BLCS['YiS'] = df[['Ti1','Ti2','Ci']].min(axis=1)\n",
    "BLCS['Di1'] = np.where(df['Ti1'] <= BLCS['Yi2'], 1, 0)\n",
    "BLCS['Di2'] = np.where(df['Ti2'] <= df['Ci'], 1, 0)\n",
    "BLCS['DiS'] = np.where(df[['Ti2','Ti2']].min(axis=1) <= df['Ci'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6302d-6e4d-46d6-9e44-69882f04acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor Recoding\n",
    "BLCS['sex'] = BLCS['sex'].map({1:'Male',2:'Female'}).astype('category')\n",
    "# Race\n",
    "race_map = {1:'White/Caucasian',2:'Native American/Alaska Native',3:'Asian',\n",
    "            4:'Black/African American',5:'Native Hawaiian/Pacific Islander',\n",
    "            6:'Multiracial',7:'Other'}\n",
    "BLCS['race'] = BLCS['race'].map(race_map).astype('category')\n",
    "BLCS['ethnic'] = BLCS['ethnic'].map({0:'Non-Hispanic',1:'Hispanic'}).astype('category')\n",
    "# Education\n",
    "edu_map = {\n",
    "    1:'Some grade school',2:'Some high school',3:'High school graduate',\n",
    "    4:\"Vocational/tech school after high school\",5:\"Some college or associate's degree\",\n",
    "    6:'College graduate',7:'Graduate or professional school',8:'Other'\n",
    "}\n",
    "BLCS['education'] = BLCS['education'].map(edu_map).astype('category')\n",
    "BLCS['bmi'] = BLCS['wtkg'] / (BLCS['htm']**2)\n",
    "BLCS['smk'] = BLCS['smk'].map({1:'Never smoker',2:'Former smoker',3:'Current smoker',4:'Smoker, status unknown'}).astype('category')\n",
    "# Treatment\n",
    "BLCS['trt'] = np.select(\n",
    "    [BLCS['surg']==1, BLCS['chemo']==1, BLCS['radio']==1, BLCS['othertrt']==1],\n",
    "    ['Surgery','Chemotherapy','Radiation','Other'], default=np.nan\n",
    ")\n",
    "# Comorbidities\n",
    "BLCS['copd'] = BLCS['copd'].map({0:'No',1:'Yes'}).astype('category')\n",
    "BLCS['asthma'] = BLCS['asthma'].map({0:'No',1:'Yes'}).astype('category')\n",
    "BLCS['egfr'] = BLCS['egfr'].map({0:'No',1:'Yes'}).fillna('Not Tested').astype('category')\n",
    "BLCS['kras'] = BLCS['kras'].map({0:'No',1:'Yes'}).fillna('Not Tested').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47bfa009-a11d-42b2-9209-c89447da0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['ptime','pevent','dtime','devent','Yi1','Yi2','YiS','Di1','Di2','DiS',\n",
    "               'agedx','sex','race','ethnic','education','bmi','smk','pkyrs','trt',\n",
    "               'ctype','cstage','copd','asthma','egfr','kras']\n",
    "BLCS_OUT4 = BLCS[final_cols + [c for c in BLCS.columns if c not in final_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "420dd830-7389-4977-94b3-b5e658994607",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLCS_CHECK = BLCS_OUT4[(BLCS_OUT4['Yi1'] <= 0) | (BLCS_OUT4['Yi2'] <= 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad0b2c2b-5444-43a1-add1-3a36ceb01629",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLCS_CLEAN = BLCS_OUT4[(BLCS_OUT4['Yi1'] > 0) & (BLCS_OUT4['Yi2'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aaf402-9072-4503-873b-d2b4eb46623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-Ready Dataset\n",
    "BLCS_CLEAN2 = BLCS_CLEAN.copy()\n",
    "BLCS_CLEAN2['stage_splt'] = np.where(BLCS_CLEAN2['cstage'].isin(['1','1A','1B','2','2A','2B','3','3A']),0,1)\n",
    "BLCS_CLEAN2['agedx'].fillna(BLCS_CLEAN2['agedx'].mean(),inplace=True)\n",
    "# Add and fill Unknown levels correctly\n",
    "BLCS_CLEAN2['sex'] = BLCS_CLEAN2['sex'].cat.add_categories('Unknown')\n",
    "BLCS_CLEAN2['sex'].fillna('Unknown',inplace=True)\n",
    "BLCS_CLEAN2['race'] = BLCS_CLEAN2['race'].cat.add_categories('Unknown')\n",
    "BLCS_CLEAN2['race'].fillna('Unknown',inplace=True)\n",
    "# Collapse race\n",
    "BLCS_CLEAN2['race'] = BLCS_CLEAN2['race'].apply(lambda r: 'White/Caucasian' if r=='White/Caucasian' else 'Other')\n",
    "BLCS_CLEAN2['education'] = BLCS_CLEAN2['education'].cat.add_categories('Unknown')\n",
    "BLCS_CLEAN2['education'].fillna('Unknown',inplace=True)\n",
    "BLCS_CLEAN2['education'] = BLCS_CLEAN2['education'].replace({'Other':'Other'})\n",
    "BLCS_CLEAN2['bmi'].fillna(BLCS_CLEAN2['bmi'].mean(),inplace=True)\n",
    "BLCS_CLEAN2['smk'] = BLCS_CLEAN2['smk'].cat.add_categories('Unknown')\n",
    "BLCS_CLEAN2['smk'].fillna('Unknown',inplace=True)\n",
    "BLCS_CLEAN2['smk'] = BLCS_CLEAN2['smk'].replace({'Smoker, status unknown':'Smoker, status unknown'})\n",
    "BLCS_CLEAN2['pkyrs'].fillna(BLCS_CLEAN2['pkyrs'].mean(),inplace=True)\n",
    "BLCS_CLEAN2['surg'] = np.where(BLCS_CLEAN2['trt']=='Surgery',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a3ed875-b7b2-4a06-9179-79e07b3cc2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptime</th>\n",
       "      <th>pevent</th>\n",
       "      <th>dtime</th>\n",
       "      <th>devent</th>\n",
       "      <th>Yi1</th>\n",
       "      <th>Yi2</th>\n",
       "      <th>YiS</th>\n",
       "      <th>Di1</th>\n",
       "      <th>Di2</th>\n",
       "      <th>DiS</th>\n",
       "      <th>...</th>\n",
       "      <th>alivedate</th>\n",
       "      <th>dead</th>\n",
       "      <th>questdate</th>\n",
       "      <th>pdate</th>\n",
       "      <th>censordate</th>\n",
       "      <th>censorcheck</th>\n",
       "      <th>Ti1</th>\n",
       "      <th>Ti2</th>\n",
       "      <th>Ci</th>\n",
       "      <th>stage_splt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350445</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350445</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350445</td>\n",
       "      <td>0.350445</td>\n",
       "      <td>0.350445</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1992-12-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1993-04-22</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.350445</td>\n",
       "      <td>0.350445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.377823</td>\n",
       "      <td>0</td>\n",
       "      <td>4.377823</td>\n",
       "      <td>1</td>\n",
       "      <td>4.377823</td>\n",
       "      <td>4.377823</td>\n",
       "      <td>4.377823</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1992-12-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1997-05-02</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.377823</td>\n",
       "      <td>4.377823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.816564</td>\n",
       "      <td>0</td>\n",
       "      <td>7.816564</td>\n",
       "      <td>1</td>\n",
       "      <td>7.816564</td>\n",
       "      <td>7.816564</td>\n",
       "      <td>7.816564</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1992-12-15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2000-10-11</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.816564</td>\n",
       "      <td>7.816564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.685832</td>\n",
       "      <td>0</td>\n",
       "      <td>2.685832</td>\n",
       "      <td>1</td>\n",
       "      <td>2.685832</td>\n",
       "      <td>2.685832</td>\n",
       "      <td>2.685832</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1992-12-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1995-08-23</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.685832</td>\n",
       "      <td>2.685832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.451745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1992-12-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1993-05-30</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686</th>\n",
       "      <td>1.201916</td>\n",
       "      <td>0</td>\n",
       "      <td>1.201916</td>\n",
       "      <td>0</td>\n",
       "      <td>1.201916</td>\n",
       "      <td>1.201916</td>\n",
       "      <td>1.201916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.201916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7688</th>\n",
       "      <td>0.418891</td>\n",
       "      <td>0</td>\n",
       "      <td>0.418891</td>\n",
       "      <td>0</td>\n",
       "      <td>0.418891</td>\n",
       "      <td>0.418891</td>\n",
       "      <td>0.418891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.418891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7691</th>\n",
       "      <td>0.123203</td>\n",
       "      <td>0</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7692</th>\n",
       "      <td>0.443532</td>\n",
       "      <td>0</td>\n",
       "      <td>0.443532</td>\n",
       "      <td>0</td>\n",
       "      <td>0.443532</td>\n",
       "      <td>0.443532</td>\n",
       "      <td>0.443532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>Not Corrected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.443532</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7702</th>\n",
       "      <td>0.553046</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.553046</td>\n",
       "      <td>0.766598</td>\n",
       "      <td>0.553046</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-10-26</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>Corrected Censoring with Death</td>\n",
       "      <td>0.553046</td>\n",
       "      <td>0.766598</td>\n",
       "      <td>0.895277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7403 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ptime  pevent     dtime  devent       Yi1       Yi2       YiS  Di1  \\\n",
       "0     0.350445       0  0.350445       1  0.350445  0.350445  0.350445    0   \n",
       "1     4.377823       0  4.377823       1  4.377823  4.377823  4.377823    0   \n",
       "2     7.816564       0  7.816564       1  7.816564  7.816564  7.816564    0   \n",
       "3     2.685832       0  2.685832       1  2.685832  2.685832  2.685832    0   \n",
       "4     0.451745       0  0.451745       1  0.451745  0.451745  0.451745    0   \n",
       "...        ...     ...       ...     ...       ...       ...       ...  ...   \n",
       "7686  1.201916       0  1.201916       0  1.201916  1.201916  1.201916    0   \n",
       "7688  0.418891       0  0.418891       0  0.418891  0.418891  0.418891    0   \n",
       "7691  0.123203       0  0.123203       0  0.123203  0.123203  0.123203    0   \n",
       "7692  0.443532       0  0.443532       0  0.443532  0.443532  0.443532    0   \n",
       "7702  0.553046       1  0.766598       1  0.553046  0.766598  0.553046    1   \n",
       "\n",
       "      Di2  DiS  ...  alivedate dead  questdate      pdate censordate  \\\n",
       "0       1    1  ...        NaT  1.0 1992-12-14        NaT 1993-04-22   \n",
       "1       1    1  ...        NaT  1.0 1992-12-14        NaT 1997-05-02   \n",
       "2       1    1  ...        NaT  1.0 1992-12-15        NaT 2000-10-11   \n",
       "3       1    1  ...        NaT  1.0 1992-12-21        NaT 1995-08-23   \n",
       "4       1    1  ...        NaT  1.0 1992-12-22        NaT 1993-05-30   \n",
       "...   ...  ...  ...        ...  ...        ...        ...        ...   \n",
       "7686    0    0  ... 2023-01-22  0.0        NaT        NaT 2023-01-22   \n",
       "7688    0    0  ... 2022-12-08  0.0 2022-12-07        NaT 2022-12-08   \n",
       "7691    0    0  ... 2022-10-14  0.0 2022-10-12        NaT 2022-10-14   \n",
       "7692    0    0  ... 2022-11-16  0.0 2022-11-15        NaT 2022-11-16   \n",
       "7702    1    1  ... 2023-01-12  1.0        NaT 2022-10-26 2023-02-28   \n",
       "\n",
       "                         censorcheck       Ti1       Ti2        Ci  stage_splt  \n",
       "0                      Not Corrected       NaN  0.350445  0.350445           0  \n",
       "1                      Not Corrected       NaN  4.377823  4.377823           0  \n",
       "2                      Not Corrected       NaN  7.816564  7.816564           1  \n",
       "3                      Not Corrected       NaN  2.685832  2.685832           0  \n",
       "4                      Not Corrected       NaN  0.451745  0.451745           0  \n",
       "...                              ...       ...       ...       ...         ...  \n",
       "7686                   Not Corrected       NaN       NaN  1.201916           1  \n",
       "7688                   Not Corrected       NaN       NaN  0.418891           1  \n",
       "7691                   Not Corrected       NaN       NaN  0.123203           0  \n",
       "7692                   Not Corrected       NaN       NaN  0.443532           0  \n",
       "7702  Corrected Censoring with Death  0.553046  0.766598  0.895277           1  \n",
       "\n",
       "[7403 rows x 74 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLCS_CLEAN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa5b740f-ebdb-439b-9ef5-f4a46c6a533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import psi, gammaln\n",
    "from scipy.special import digamma\n",
    "import pyreadr\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c49f021-773c-4101-a0db-e5c410781b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1_fun_stable(h1, d1, y1, gamma):\n",
    "    h1 = h1.view(-1)\n",
    "    d1 = d1.view(-1)\n",
    "    y1 = y1.view(-1)\n",
    "    gamma = gamma.view(-1)\n",
    "\n",
    "    risk_matrix = (y1.view(-1, 1) <= y1.view(1, -1)).float()\n",
    "\n",
    "    h1_stable = h1 - torch.max(h1)\n",
    "    exp_h1 = gamma * torch.exp(h1_stable)\n",
    "\n",
    "    denom = torch.matmul(risk_matrix, exp_h1) + 1e-8\n",
    "    log_risk = torch.log(denom) + torch.max(h1)\n",
    "\n",
    "    loss_vector = -d1 * (h1 - log_risk)\n",
    "\n",
    "    return loss_vector.sum() / (d1.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63f2c527-58b6-448d-bf17-9fc68817233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss2_fun_stable(h2, d1, d2, y1, gamma):\n",
    "    h2 = h2.view(-1)\n",
    "    d1 = d1.view(-1)\n",
    "    d2 = d2.view(-1)\n",
    "    y1 = y1.view(-1)\n",
    "    gamma = gamma.view(-1)\n",
    "\n",
    "    event_mask = ((d1 == 0) & (d2 == 1)).float()\n",
    "    risk_matrix = (y1.view(-1, 1) <= y1.view(1, -1)).float()\n",
    "\n",
    "    h2_stable = h2 - torch.max(h2)\n",
    "    exp_h2 = gamma * torch.exp(h2_stable)\n",
    "\n",
    "    denom = torch.matmul(risk_matrix, exp_h2) + 1e-8\n",
    "    log_risk = torch.log(denom) + torch.max(h2)\n",
    "\n",
    "    loss_vector = -event_mask * (h2 - log_risk)\n",
    "\n",
    "    return loss_vector.sum() / (event_mask.sum() + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30ae50c9-869b-4e40-8c69-d3109e4e28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss3_fun_stable(h3, d1, d2, y1, y2, gamma):\n",
    "    h3 = h3.view(-1)\n",
    "    d1 = d1.view(-1)\n",
    "    d2 = d2.view(-1)\n",
    "    y1 = y1.view(-1)\n",
    "    y2 = y2.view(-1)\n",
    "    gamma = gamma.view(-1)\n",
    "\n",
    "    event_mask = ((d1 == 1) & (d2 == 1)).float()\n",
    "\n",
    "    risk_matrix = ((y1.view(1, -1) < y2.view(-1, 1)) &\n",
    "                   (y2.view(1, -1) >= y2.view(-1, 1))).float()\n",
    "\n",
    "    h3_stable = h3 - torch.max(h3)\n",
    "    exp_h3 = gamma * d1 * torch.exp(h3_stable)\n",
    "\n",
    "    denom = torch.matmul(risk_matrix, exp_h3) + 1e-8\n",
    "    log_risk = torch.log(denom) + torch.max(h3)\n",
    "\n",
    "    loss_vector = -event_mask * (h3 - log_risk)\n",
    "\n",
    "    return loss_vector.sum() / (event_mask.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad10e4-0459-480b-9b20-311036f5dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dnn(formula, data, na_action=\"na.fail\", subset=None,\n",
    "            dim_layers=[128, 64, 16], lr=0.01, dr=0.1,\n",
    "            max_epochs=250, max_epochs_theta=100, max_epochs_n=5, verbose=True,\n",
    "            ll=1, tol=1e-6, theta0=0.5, lr_theta=0.01, batch_size=128):\n",
    "    # Assertions\n",
    "    if na_action not in [\"na.fail\", \"na.omit\"]:\n",
    "        raise ValueError('na_action should be either \"na.fail\" or \"na.omit\"')\n",
    "\n",
    "    # Pre-Process Data\n",
    "    \n",
    "    # Outcomes\n",
    "    y1 = data['Yi1'].values\n",
    "    d1 = data['Di1'].values\n",
    "    y2 = data['Yi2'].values\n",
    "    d2 = data['Di2'].values\n",
    "\n",
    "    # Unique Failure Times by Transition\n",
    "    t1_obs = y1[d1 == 1]\n",
    "    t2_obs = y2[(d1 == 0) & (d2 == 1)]\n",
    "    t3_obs = y2[(d1 == 1) & (d2 == 1)]\n",
    "    t1 = np.unique(np.sort(t1_obs))\n",
    "    t2 = np.unique(np.sort(t2_obs))\n",
    "    t3 = np.unique(np.sort(t3_obs))\n",
    "    tol_val = tol\n",
    "\n",
    "    # Features\n",
    "    X1_mat = data[form['X']].values\n",
    "    X2_mat = data[form['X']].values\n",
    "    X3_mat = data[form['X']].values\n",
    "    n = X1_mat.shape[0]\n",
    "\n",
    "    # Initialize Parameters\n",
    "    theta = torch.tensor(theta0, requires_grad=True, dtype=torch.float32)\n",
    "    optimizer_theta = optim.Adam([theta], lr=lr_theta)\n",
    "\n",
    "    # Baseline Hazards\n",
    "    d1_j = np.array([np.sum(d1 * (y1 == t)) for t in t1])\n",
    "    n1_j = np.array([np.sum(y1 >= t) for t in t1])\n",
    "    lam01 = d1_j / n1_j\n",
    "\n",
    "    d2_j = np.array([np.sum((1 - d1) * d2 * (y2 == t)) for t in t2])\n",
    "    n2_j = np.array([np.sum((y2 >= t) & (y1 >= t)) for t in t2])\n",
    "    lam02 = d2_j / n2_j\n",
    "\n",
    "    d3_j = np.array([np.sum(d1 * d2 * (y2 == t)) for t in t3])\n",
    "    n3_j = np.array([np.sum(y2[d1==1] >= t) for t in t3])\n",
    "    lam03 = d3_j / n3_j\n",
    "\n",
    "    # Neural Network Sub-Architectures\n",
    "    input_dim = X1_mat.shape[1]\n",
    "    def build_model(input_dim, dim_layers, dropout):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_layers[0], bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_layers[0], dim_layers[1], bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_layers[1], dim_layers[2], bias=False),\n",
    "            nn.Linear(dim_layers[2], 1, bias=False)\n",
    "        )\n",
    "    h1_model = build_model(input_dim, dim_layers, dr)\n",
    "    h2_model = build_model(input_dim, dim_layers, dr)\n",
    "    h3_model = build_model(input_dim, dim_layers, dr)\n",
    "    optimizer_h1 = optim.Adam(h1_model.parameters(), lr=lr)\n",
    "    optimizer_h2 = optim.Adam(h2_model.parameters(), lr=lr)\n",
    "    optimizer_h3 = optim.Adam(h3_model.parameters(), lr=lr)\n",
    "\n",
    "    # Initial Risk Function Values\n",
    "    h1 = h1_model(torch.tensor(X1_mat, dtype=torch.float32))\n",
    "    h2 = h2_model(torch.tensor(X2_mat, dtype=torch.float32))\n",
    "    h3 = h3_model(torch.tensor(X3_mat, dtype=torch.float32))\n",
    "\n",
    "    # Internal Helper Functions for Baseline Hazards\n",
    "    def lam01_fun(t, gamma_val, h1_vals):\n",
    "        numer = np.sum(d1 * (y1 == t))\n",
    "        h1_np = h1_vals.detach().cpu().numpy().flatten()\n",
    "        denom = np.sum(gamma_val * ((y1 >= t).astype(float)) * np.exp(h1_np))\n",
    "        return numer/denom if numer > 0 else 0\n",
    "\n",
    "    def lam02_fun(t, gamma_val, h2_vals):\n",
    "        numer = np.sum((1 - d1) * d2 * (y1 == t))\n",
    "        h2_np = h2_vals.detach().cpu().numpy().flatten()\n",
    "        denom = np.sum(gamma_val * (((y2 >= t) & (y1 >= t)).astype(float)) * np.exp(h2_np))\n",
    "        return numer/denom if numer > 0 else 0\n",
    "\n",
    "    def lam03_fun(t, gamma_val, h3_vals):\n",
    "        numer = np.sum(d1 * d2 * (y2 == t))\n",
    "        h3_np = h3_vals.detach().cpu().numpy().flatten()\n",
    "        denom = np.sum(gamma_val * d1 * ((y2 >= t).astype(float) - (y1 >= t).astype(float)) * np.exp(h3_np))\n",
    "        return numer/denom if numer > 0 else 0\n",
    "    \n",
    "    def Lam01_fun(t, lam01):\n",
    "        return np.sum(lam01[t1 - t < tol_val])\n",
    "    def Lam02_fun(t, lam02):\n",
    "        return np.sum(lam02[t2 - t < tol_val])\n",
    "    def Lam03_fun(t, lam03):\n",
    "        return np.sum(lam03[t3 - t < tol_val])\n",
    "\n",
    "    # Theta Contribution to Expected Log-Likelihood\n",
    "    def loss4(theta_tensor, gamma_tensor, log_gamma_tensor):\n",
    "        loss = -1/theta_tensor * torch.log(theta_tensor) + (1/theta_tensor - 1)*log_gamma_tensor \\\n",
    "               - 1/theta_tensor * gamma_tensor - torch.lgamma(1/theta_tensor)\n",
    "        return -torch.sum(loss)\n",
    "\n",
    "    # Dataset Class for Dataloader\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, inputs, outputs):\n",
    "            self.inputs = inputs\n",
    "            self.outputs = outputs\n",
    "        def __len__(self):\n",
    "            return self.inputs.shape[0]\n",
    "        def __getitem__(self, idx):\n",
    "            return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "    # Neural EM-Algorithm Loop\n",
    "\n",
    "    # Initialize Loss Tracking\n",
    "    loss_h1_epoch = [np.inf]*(max_epochs+1)\n",
    "    loss_h2_epoch = [np.inf]*(max_epochs+1)\n",
    "    loss_h3_epoch = [np.inf]*(max_epochs+1)\n",
    "    loss_theta_epoch = [np.inf]*(max_epochs+1)\n",
    "    \n",
    "    loss_theta_inc = 0\n",
    "    loss_h1_inc = 0\n",
    "    loss_h2_inc = 0\n",
    "    loss_h3_inc = 0\n",
    "    gamma_val = np.ones_like(y1)\n",
    "    log_gamma_val = np.zeros_like(y1)\n",
    "    diff_EM = 100\n",
    "    epoch = 2\n",
    "\n",
    "    while epoch < max_epochs and diff_EM > tol_val:\n",
    "        # E-STEP\n",
    "        theta_val = theta.item()\n",
    "        a = 1/theta_val + d1 + d2 \n",
    "        # Update Risk Functions\n",
    "        h1_all = h1_model(torch.tensor(X1_mat, dtype=torch.float32)).detach()\n",
    "        h2_all = h2_model(torch.tensor(X2_mat, dtype=torch.float32)).detach()\n",
    "        h3_all = h3_model(torch.tensor(X3_mat, dtype=torch.float32)).detach()\n",
    "        # Compute Cumulative Hazards for Each Subject\n",
    "        Lam01 = np.array([Lam01_fun(t, lam01) for t in y1])\n",
    "        Lam02 = np.array([Lam02_fun(t, lam02) for t in y1])\n",
    "        Lam03_y2 = np.array([Lam03_fun(t, lam03) for t in y2])\n",
    "        Lam03_y1 = np.array([Lam03_fun(t, lam03) for t in y1])\n",
    "        h1_np = h1_all.numpy().flatten()\n",
    "        h2_np = h2_all.numpy().flatten()\n",
    "        h3_np = h3_all.numpy().flatten()\n",
    "        b = 1/theta_val + Lam01 * np.exp(h1_np) + Lam02 * np.exp(h2_np) + d1 * (Lam03_y2 - Lam03_y1) * np.exp(h3_np)\n",
    "        gamma_val = a / b\n",
    "        log_gamma_val = digamma(a) - np.log(b)\n",
    "\n",
    "        print(\"mean:\", np.mean(gamma_val))\n",
    "        print(\"var:\", np.var(gamma_val))\n",
    "\n",
    "        # M-STEP\n",
    "        lam01_new = np.array([lam01_fun(t, gamma_val, h1_all) for t in t1])\n",
    "        lam02_new = np.array([lam02_fun(t, gamma_val, h2_all) for t in t2])\n",
    "        lam03_new = np.array([lam03_fun(t, gamma_val, h3_all) for t in t3])\n",
    "        diff_EM = max(np.max(np.abs((lam01 - lam01_new) / (lam01 + 1e-8))),\n",
    "                    np.max(np.abs((lam02 - lam02_new) / (lam02 + 1e-8))),\n",
    "                    np.max(np.abs((lam03 - lam03_new) / (lam03 + 1e-8))))\n",
    "        lam01 = lam01_new\n",
    "        lam02 = lam02_new\n",
    "        lam03 = lam03_new\n",
    "\n",
    "        # N-STEP\n",
    "        loss_theta_inc_inner = 0\n",
    "        prev_loss_theta_inner = float('inf')\n",
    "        \n",
    "        if loss_theta_inc < 2:\n",
    "            for epoch_theta in range(max_epochs_theta):\n",
    "                optimizer_theta.zero_grad()\n",
    "                loss_theta = loss4(theta,\n",
    "                                     torch.tensor(gamma_val, dtype=torch.float32),\n",
    "                                     torch.tensor(log_gamma_val, dtype=torch.float32))\n",
    "                loss_theta.backward(retain_graph=True)\n",
    "                optimizer_theta.step()\n",
    "                current_loss_theta = loss_theta.item()\n",
    "                if abs(current_loss_theta - prev_loss_theta_inner) <= 1e-4:\n",
    "                    break\n",
    "                \n",
    "                elif current_loss_theta > prev_loss_theta_inner:\n",
    "                    loss_theta_inc_inner += 1\n",
    "                    if loss_theta_inc_inner > 2:\n",
    "                        break\n",
    "                if verbose:\n",
    "                    print(f\"Theta Epoch {epoch_theta}, Loss: {current_loss_theta}, Theta: {theta.item()}\")\n",
    "                \n",
    "                prev_loss_theta_inner = current_loss_theta\n",
    "            \n",
    "            if current_loss_theta > loss_theta_epoch[epoch-1]:\n",
    "                loss_theta_inc += 1\n",
    "            else:\n",
    "                loss_theta_inc = 0\n",
    "            \n",
    "            loss_theta_epoch[epoch] = current_loss_theta\n",
    "\n",
    "        # Update Neural Network Parameters\n",
    "        # Create Datasets for Each Transition\n",
    "        outputs1 = np.column_stack((y1, d1, y2, d2, gamma_val))\n",
    "        dataset1 = CustomDataset(torch.tensor(X1_mat, dtype=torch.float32),\n",
    "                                 torch.tensor(outputs1, dtype=torch.float32))\n",
    "        dl1 = DataLoader(dataset1, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        outputs2 = np.column_stack((y1, d1, y2, d2, gamma_val))\n",
    "        dataset2 = CustomDataset(torch.tensor(X2_mat, dtype=torch.float32),\n",
    "                                 torch.tensor(outputs2, dtype=torch.float32))\n",
    "        dl2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        outputs3 = np.column_stack((y1, d1, y2, d2, gamma_val))\n",
    "        dataset3 = CustomDataset(torch.tensor(X3_mat, dtype=torch.float32),\n",
    "                                 torch.tensor(outputs3, dtype=torch.float32))\n",
    "        dl3 = DataLoader(dataset3, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        loss1_val, loss2_val, loss3_val = float('inf'), float('inf'), float('inf')\n",
    "        \n",
    "\n",
    "        loss_h1_inc_inner = 0\n",
    "        prev_loss_h1_inner = float('inf')\n",
    "        \n",
    "        if loss_h1_inc < 2:\n",
    "            \n",
    "            for epoch_h1 in range(max_epochs_n):\n",
    "            \n",
    "                loss_h1_epoch_n = 0\n",
    "\n",
    "                num_batches1 = 0\n",
    "\n",
    "                # Transition 1\n",
    "                for batch_X1, batch_out in dl1:\n",
    "                    num_batches1 += 1\n",
    "                    batch_y1 = batch_out[:, 0]\n",
    "                    batch_d1 = batch_out[:, 1]\n",
    "                    batch_y2 = batch_out[:, 2]\n",
    "                    batch_d2 = batch_out[:, 3]\n",
    "                    batch_gamma = batch_out[:, 4]\n",
    "                    h1_batch = h1_model(batch_X1)\n",
    "                    event_idx = (batch_d1 == 1).nonzero(as_tuple=True)[0]\n",
    "                    loss_h1 = 0.0\n",
    "                    if len(event_idx) > 0:\n",
    "                        loss_h1 = loss1_fun_stable(h1_batch, batch_d1, batch_y1, batch_gamma)\n",
    "                        optimizer_h1.zero_grad()\n",
    "                        loss_h1.backward(retain_graph=True)\n",
    "                        optimizer_h1.step()\n",
    "\n",
    "                    current_loss_h1 = loss_h1.item()\n",
    "                \n",
    "                if num_batches1 > 0:\n",
    "                    \n",
    "                    current_loss_h1 = current_loss_h1 / num_batches1\n",
    "                    \n",
    "                if abs(current_loss_h1 - prev_loss_h1_inner) <= 1e-4:\n",
    "                    break\n",
    "                \n",
    "                elif current_loss_h1 > prev_loss_h1_inner:\n",
    "                        \n",
    "                    loss_h1_inc_inner += 1\n",
    "                    \n",
    "                    if loss_h1_inc_inner > 2:\n",
    "                            \n",
    "                        break\n",
    "                    \n",
    "                prev_loss_h1_inner = current_loss_h1\n",
    "                       \n",
    "            loss1_val = current_loss_h1\n",
    "            \n",
    "            if loss1_val > loss_h1_epoch[epoch-1]:\n",
    "                \n",
    "                loss_h1_inc += 1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                loss_h1_inc = 0\n",
    "\n",
    "            loss_h1_epoch[epoch] = loss1_val\n",
    "            \n",
    "            h1 = h1_model(torch.tensor(X1_mat, dtype=torch.float32))\n",
    "            \n",
    "        loss_h2_inc_inner = 0\n",
    "        prev_loss_h2_inner = float('inf')\n",
    "        \n",
    "        if loss_h2_inc < 2:\n",
    "            \n",
    "            for epoch_h2 in range(max_epochs_n):\n",
    "            \n",
    "                loss_h2_epoch_n = 0\n",
    "\n",
    "                num_batches2 = 0\n",
    "\n",
    "                # Transition 2\n",
    "                for batch_X2, batch_out in dl2:\n",
    "                    num_batches2 += 1\n",
    "                    batch_y1 = batch_out[:, 0]\n",
    "                    batch_d1 = batch_out[:, 1]\n",
    "                    batch_y2 = batch_out[:, 2]\n",
    "                    batch_d2 = batch_out[:, 3]\n",
    "                    batch_gamma = batch_out[:, 4]\n",
    "                    h2_batch = h2_model(batch_X2)\n",
    "                    event_idx = ((batch_d1 == 0) & (batch_d2 == 1)).nonzero(as_tuple=True)[0]\n",
    "                    loss_h2 = 0.0\n",
    "                    if len(event_idx) > 0:\n",
    "                        loss_h2 = loss2_fun_stable(h2_batch, batch_d1, batch_d2, batch_y1, batch_gamma)\n",
    "                        optimizer_h2.zero_grad()\n",
    "                        loss_h2.backward(retain_graph=True)\n",
    "                        optimizer_h2.step()\n",
    "\n",
    "                    current_loss_h2 = loss_h2.item()\n",
    "                \n",
    "                if num_batches2 > 0:\n",
    "                    \n",
    "                    current_loss_h2 = current_loss_h2 / num_batches2\n",
    "                    \n",
    "                if abs(current_loss_h2 - prev_loss_h2_inner) <= 1e-4:\n",
    "                    break\n",
    "                \n",
    "                elif current_loss_h2 > prev_loss_h2_inner:\n",
    "                        \n",
    "                    loss_h2_inc_inner += 1\n",
    "                    \n",
    "                    if loss_h2_inc_inner > 2:\n",
    "                            \n",
    "                        break\n",
    "                    \n",
    "                prev_loss_h2_inner = current_loss_h2\n",
    "                       \n",
    "            loss2_val = current_loss_h2\n",
    "            \n",
    "            if loss2_val > loss_h2_epoch[epoch-1]:\n",
    "                \n",
    "                loss_h2_inc += 1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                loss_h2_inc = 0\n",
    "\n",
    "            loss_h2_epoch[epoch] = loss2_val\n",
    "            \n",
    "            h2 = h2_model(torch.tensor(X2_mat, dtype=torch.float32))\n",
    "            \n",
    "        loss_h3_inc_inner = 0\n",
    "        prev_loss_h3_inner = float('inf')\n",
    "        \n",
    "        if loss_h3_inc < 2:\n",
    "            \n",
    "            for epoch_h3 in range(max_epochs_n):\n",
    "            \n",
    "                loss_h3_epoch_n = 0\n",
    "\n",
    "                num_batches3 = 0\n",
    "\n",
    "                # Transition 3\n",
    "                for batch_X3, batch_out in dl3:\n",
    "                    num_batches3 += 1\n",
    "                    batch_y1 = batch_out[:, 0]\n",
    "                    batch_d1 = batch_out[:, 1]\n",
    "                    batch_y2 = batch_out[:, 2]\n",
    "                    batch_d2 = batch_out[:, 3]\n",
    "                    batch_gamma = batch_out[:, 4]\n",
    "                    h3_batch = h3_model(batch_X3)\n",
    "                    event_idx = ((batch_d1 == 1) & (batch_d2 == 1)).nonzero(as_tuple=True)[0]\n",
    "                    loss_h3 = 0.0\n",
    "                    if len(event_idx) > 0:\n",
    "                        loss_h3 = loss3_fun_stable(h3_batch, batch_d1, batch_d2, batch_y1, batch_y2, batch_gamma)\n",
    "                        optimizer_h3.zero_grad()\n",
    "                        loss_h3.backward(retain_graph=True)\n",
    "                        optimizer_h3.step()\n",
    "                \n",
    "                    current_loss_h3 = loss_h3.item()\n",
    "                \n",
    "                if num_batches3 > 0:\n",
    "                    \n",
    "                    current_loss_h3 = current_loss_h3 / num_batches3\n",
    "                    \n",
    "                if abs(current_loss_h3 - prev_loss_h3_inner) <= 1e-4:\n",
    "                    break\n",
    "                \n",
    "                elif current_loss_h3 > prev_loss_h3_inner:\n",
    "                        \n",
    "                    loss_h3_inc_inner += 1\n",
    "                    \n",
    "                    if loss_h3_inc_inner > 2:\n",
    "                            \n",
    "                        break\n",
    "                    \n",
    "                prev_loss_h3_inner = current_loss_h3\n",
    "                       \n",
    "            loss3_val = current_loss_h3\n",
    "            \n",
    "            if loss3_val > loss_h3_epoch[epoch-1]:\n",
    "                \n",
    "                loss_h3_inc += 1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                loss_h3_inc = 0\n",
    "\n",
    "            loss_h3_epoch[epoch] = loss3_val\n",
    "            \n",
    "            h3 = h3_model(torch.tensor(X3_mat, dtype=torch.float32))\n",
    "        \n",
    "\n",
    "        # Plot Baseline Hazards\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(t1, np.cumsum(lam01), marker='o')\n",
    "        plt.title('Baseline Hazards 1')\n",
    "        plt.xlabel('Failure Time')\n",
    "        plt.ylabel('Cumulative Baseline Hazards')\n",
    "        plt.axline((0, 0), slope=2, linestyle='--')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(t2, np.cumsum(lam02), marker='o')\n",
    "        plt.title('Baseline Hazards 2')\n",
    "        plt.xlabel('Failure Time')\n",
    "        plt.ylabel('Cumulative Baseline Hazards')\n",
    "        plt.axline((0, 0), slope=3, linestyle='--')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(t3, np.cumsum(lam03), marker='o')\n",
    "        plt.title('Baseline Hazards 3')\n",
    "        plt.xlabel('Failure Time')\n",
    "        plt.ylabel('Cumulative Baseline Hazards')\n",
    "        plt.axline((0, 0), slope=2, linestyle='--')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss1: {loss1_val}, loss2: {loss2_val}, loss3: {loss3_val}\")\n",
    "        epoch += 1\n",
    "        diff_EM = 100  # Reset diff_EM for next outer iteration\n",
    "\n",
    "        epochs = range(1, len(loss_h1_epoch))\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.plot(epochs, loss_h1_epoch[1:], label=\"Loss h1\")\n",
    "        plt.plot(epochs, loss_h2_epoch[1:], label=\"Loss h2\")\n",
    "        plt.plot(epochs, loss_h3_epoch[1:], label=\"Loss h3\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Curves\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if loss_h1_inc >= 2 and loss_h2_inc >= 2 and loss_h3_inc >=2:\n",
    "            \n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"theta\": theta.item(),\n",
    "        \"lam01\": lam01,\n",
    "        \"lam02\": lam02,\n",
    "        \"lam03\": lam03,\n",
    "        \"h1\": h1,\n",
    "        \"h2\": h2,\n",
    "        \"h3\": h3,\n",
    "        \"h1_model\": h1_model,\n",
    "        \"h2_model\": h2_model,\n",
    "        \"h3_model\": h3_model,\n",
    "        \"gamma\": gamma_val\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5dd41-afb7-40c8-89a2-686e690e1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(dat_sim, form):\n",
    "    \n",
    "    fit_00 = fit_dnn(form, dat_sim,\n",
    "                     na_action=\"na.fail\", subset=None, dim_layers=[1024, 128, 16],\n",
    "                     lr=0.001, dr=0.3,\n",
    "                     max_epochs=5, max_epochs_theta=20, max_epochs_n=100,\n",
    "                     batch_size=500, verbose=True, ll=1, tol=1e-6,\n",
    "                     theta0=1, lr_theta=0.05)\n",
    "    \n",
    "    result = {\n",
    "    \"theta\": fit_00[\"theta\"],\n",
    "    \"lam01\": fit_00[\"lam01\"],\n",
    "    \"lam02\": fit_00[\"lam02\"],\n",
    "    \"lam03\": fit_00[\"lam03\"],\n",
    "    \"h1\": fit_00[\"h1\"].detach().numpy(),\n",
    "    \"h2\": fit_00[\"h2\"].detach().numpy(),\n",
    "    \"h3\": fit_00[\"h3\"].detach().numpy(),\n",
    "    \"h1_model\": fit_00[\"h1_model\"],\n",
    "    \"h2_model\": fit_00[\"h2_model\"],\n",
    "    \"h3_model\": fit_00[\"h3_model\"],\n",
    "    \"gamma\": fit_00[\"gamma\"]\n",
    "}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262d9f4-dd99-4f0c-a3ac-81de26471e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = BLCS_CLEAN2[['Yi1','Yi2','YiS','Di1','Di2','DiS','agedx','sex','race','ethnic','smk','pkyrs','trt','egfr','kras','copd','asthma','stage_splt']]\n",
    "\n",
    "dat = pd.get_dummies(dat, drop_first=True)*1\n",
    "form = {\n",
    "    'Y': [\"Yi1\", \"Di1\", \"Yi2\", \"Di2\"],\n",
    "    'X': ['agedx', 'pkyrs', 'sex_Male',\n",
    "       'sex_Unknown', 'race_White/Caucasian', 'ethnic_Non-Hispanic',\n",
    "       'smk_Former smoker', 'smk_Never smoker', 'smk_Smoker, status unknown',\n",
    "       'smk_Unknown', 'trt_Other', 'trt_Radiation', 'trt_Surgery', 'trt_nan',\n",
    "       'egfr_Not Tested', 'egfr_Yes', 'kras_Not Tested', 'kras_Yes',\n",
    "       'copd_Yes', 'asthma_Yes','stage_splt']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd6dcc-3582-411f-8398-7b7be3b27d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5d23c-817c-4875-9258-a92378b83ac4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_simulation(dat, form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecdb54-607e-4dcc-94fc-374ca03e4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbs(data, preds, t_pred):\n",
    "    y1 = data['Yi1'].values\n",
    "    d1 = data['Di1'].astype(bool).values\n",
    "    y2 = data['Yi2'].values\n",
    "    d2 = data['Di2'].astype(bool).values\n",
    "    ctime = data['YiS'].values\n",
    "    cind = data['DiS'].astype(bool).values\n",
    "    \n",
    "    t_pred = np.asarray(t_pred)\n",
    "    n, m = preds.shape\n",
    "    \n",
    "    ind_y1 = y1[:, None] <= t_pred[None, :]\n",
    "    ind_y2 = y2[:, None] <= t_pred[None, :]\n",
    "    ind_y1_y2 = (y1 <= y2)  # shape (n,)\n",
    "    \n",
    "    ind_cat1 = (ind_y1 & d1[:, None] & ind_y1_y2[:, None]).astype(int)\n",
    "    ind_cat2 = (ind_y1 & ind_y2 & (~d1[:, None]) & d2[:, None] & ind_y1_y2[:, None]).astype(int)\n",
    "    ind_cat3 = ((~ind_y1) & (~ind_y2)).astype(int)\n",
    "    \n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(durations=ctime, event_observed=cind)\n",
    "    \n",
    "    csurv = kmf.survival_function_at_times(ctime).values\n",
    "    csurv[csurv == 0] = np.inf\n",
    "    \n",
    "    csurv_pred = kmf.survival_function_at_times(t_pred).values\n",
    "    min_valid = np.nanmin(csurv_pred)\n",
    "    csurv_pred = np.where(np.isnan(csurv_pred), min_valid, csurv_pred)\n",
    "    csurv_pred[csurv_pred == 0] = np.inf\n",
    "\n",
    "    bs = np.zeros(m)\n",
    "    for j in range(m):\n",
    "        bs[j] = np.mean(\n",
    "            (0 - preds[:, j])**2 * ind_cat1[:, j] * (1 / csurv) +\n",
    "            (0 - preds[:, j])**2 * ind_cat2[:, j] * (1 / csurv) +\n",
    "            (1 - preds[:, j])**2 * ind_cat3[:, j] * (1 / csurv_pred[j])\n",
    "        )\n",
    "\n",
    "    if m > 1:\n",
    "        dt = np.diff(t_pred)\n",
    "        bs_mid = (bs[:-1] + bs[1:]) / 2\n",
    "        ibs = np.dot(dt, bs_mid) / (t_pred.max() - t_pred.min())\n",
    "    else:\n",
    "        ibs = bs[0] / t_pred[0]\n",
    "    \n",
    "    return {'tpred': t_pred, 'bs': bs, 'ibs': ibs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f7736-70a4-429f-8637-35ae5c928927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "t_pred = np.linspace(0, 5, 100)\n",
    "\n",
    "bs_mat = np.full((len(t_pred), K), np.nan)\n",
    "ibs_vec = np.empty(K)\n",
    "\n",
    "for k, (train_idx, test_idx) in enumerate(kf.split(dat)):\n",
    "    train_dat = dat.iloc[train_idx].copy()\n",
    "    test_dat = dat.iloc[test_idx].copy()\n",
    "\n",
    "    y1 = test_dat['Yi1'].values\n",
    "    d1 = test_dat['Di1'].values\n",
    "    y2 = test_dat['Yi2'].values\n",
    "    d2 = test_dat['Di2'].values\n",
    "    \n",
    "    \n",
    "    X1_mat = test_dat[form['X']].values\n",
    "    X2_mat = test_dat[form['X']].values\n",
    "    X3_mat = test_dat[form['X']].values\n",
    "    \n",
    "    \n",
    "    t1 = np.sort(y1[d1 == 1])\n",
    "    t2 = np.sort(y2[(d1 == 0) & (d2 == 1)])\n",
    "    t3 = np.sort(y2[(d1 == 1) & (d2 == 1)])\n",
    "    \n",
    "    t_max_new = max(np.max(y1), np.max(y2))\n",
    "    t_new = np.linspace(0, t_max_new, test_dat.shape[0])\n",
    "    \n",
    "    y1_old = train_dat['Yi1'].values\n",
    "    d1_old = train_dat['Di1'].values\n",
    "    y2_old = train_dat['Yi2'].values\n",
    "    d2_old = train_dat['Di2'].values\n",
    "    \n",
    "    \n",
    "    t1_old = np.sort(y1_old[d1_old == 1])\n",
    "    t2_old = np.sort(y2_old[(d1_old == 0) & (d2_old == 1)])\n",
    "    t3_old = np.sort(y2_old[(d1_old == 1) & (d2_old == 1)])\n",
    "    \n",
    "    t_max_old = max(np.max(y1_old), np.max(y2_old))\n",
    "    t_old = np.linspace(0, t_max_old, train_dat.shape[0])\n",
    "\n",
    "    res = run_simulation(train_dat, form)\n",
    "    \n",
    "\n",
    "    # Post-Estimation\n",
    "    \n",
    "    h1_model = res[\"h1_model\"]\n",
    "    h2_model = res[\"h2_model\"]\n",
    "    h3_model = res[\"h3_model\"]\n",
    " \n",
    "    lam01 = np.cumsum(res[\"lam01\"])\n",
    "    lam02 = np.cumsum(res[\"lam02\"])\n",
    "    theta = res[\"theta\"]\n",
    "    h1_pred = res[\"h1\"].flatten()\n",
    "    h2_pred = res[\"h2\"].flatten()\n",
    "    h3_pred = res[\"h3\"].flatten()\n",
    "    \n",
    "    h1_new = h1_model(torch.tensor(X1_mat, dtype=torch.float32)).detach().flatten()\n",
    "    h2_new = h2_model(torch.tensor(X2_mat, dtype=torch.float32)).detach().flatten()\n",
    "    h3_new = h3_model(torch.tensor(X3_mat, dtype=torch.float32)).detach().flatten()\n",
    "\n",
    "    preds_true = np.empty((len(t_pred), test_dat.shape[0]))\n",
    "    for idx, t in enumerate(t_pred):\n",
    "        # find the index a minimizing |t1[a] - t|          \n",
    "        a = np.argmin(np.abs(t1 - t))\n",
    "        b = np.argmin(np.abs(t2 - t))\n",
    "        # compute the formula\n",
    "        preds_true[idx, :] = (\n",
    "            1\n",
    "            + theta * lam01[a] * np.exp(h1_new)\n",
    "            + theta * lam02[b] * np.exp(h2_new)\n",
    "        ) ** (-1 / theta)\n",
    "            \n",
    "\n",
    "    res_bbs = bbs(test_dat, preds_true.T, t_pred)\n",
    "    bs_mat[:, k] = res_bbs['bs']\n",
    "    ibs_vec[k] = res_bbs['ibs']\n",
    "\n",
    "# Aggregate\n",
    "mean_bs = np.mean(bs_mat, axis=1)\n",
    "sd_bs = np.std(bs_mat, axis=1, ddof=1)\n",
    "se_bs = sd_bs / np.sqrt(K)\n",
    "from scipy.stats import t\n",
    "t_q = t.ppf(0.975, df=K-1)\n",
    "\n",
    "ci_lower = mean_bs - t_q * se_bs\n",
    "ci_upper = mean_bs + t_q * se_bs\n",
    "\n",
    "# Plot\n",
    "plt.plot(t_pred, mean_bs, lw=2, label=\"Mean BBS\")\n",
    "plt.fill_between(t_pred, ci_lower, ci_upper, alpha=0.2, color=\"blue\")\n",
    "plt.xlabel(\"Time (yrs)\")\n",
    "plt.ylabel(\"BBS\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Integrated BBS\n",
    "mean_ibs = np.mean(ibs_vec)\n",
    "se_ibs = np.std(ibs_vec, ddof=1) / np.sqrt(K)\n",
    "ci_ibs = mean_ibs + np.array([-1, 1]) * t_q * se_ibs\n",
    "\n",
    "print(f\"iBBS = {mean_ibs:.4f} (95% CI: {ci_ibs[0]:.4f} â€“ {ci_ibs[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fa081-8bef-416f-8074-1cd2da35bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_pred, mean_bs, lw=2, label=\"Mean BBS\")\n",
    "plt.fill_between(t_pred, ci_lower, ci_upper, alpha=0.2, color=\"blue\")\n",
    "bbs = {'t_pred':t_pred,\n",
    "       'mean_bs':mean_bs,\n",
    "       'ci_lower':ci_lower,\n",
    "       'ci_upper':ci_upper}\n",
    "bbs = pd.DataFrame(bbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "35e98238-c969-4c24-9f3c-0b0a7394d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs.to_csv(\"bbs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a8b01-ef53-4306-82f4-8dd63983cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def bootstrap_cumhaz(data, n_boot=50):\n",
    "    lam01_list = []\n",
    "    lam02_list = []\n",
    "    lam03_list = []\n",
    "\n",
    "    for _ in tqdm(range(n_boot)):\n",
    "        # Bootstrap resample\n",
    "        data_boot = data.sample(n=len(data), replace=True)\n",
    "\n",
    "        # Train model on bootstrap sample\n",
    "        result = run_simulation(data_boot, form)\n",
    "        \n",
    "        # Store cumulative hazard\n",
    "        lam01_list.append(np.cumsum(result[\"lam01\"]))\n",
    "        lam02_list.append(np.cumsum(result[\"lam02\"]))\n",
    "        lam03_list.append(np.cumsum(result[\"lam03\"]))\n",
    "\n",
    "    lam01_arr = np.array(lam01_list)\n",
    "    lam02_arr = np.array(lam02_list)\n",
    "    lam03_arr = np.array(lam03_list)\n",
    "\n",
    "    def mean_ci(arr):\n",
    "        mean = np.mean(arr, axis=0)\n",
    "        lower = np.percentile(arr, 2.5, axis=0)\n",
    "        upper = np.percentile(arr, 97.5, axis=0)\n",
    "        return mean, lower, upper\n",
    "\n",
    "    lam01_mean, lam01_lower, lam01_upper = mean_ci(lam01_arr)\n",
    "    lam02_mean, lam02_lower, lam02_upper = mean_ci(lam02_arr)\n",
    "    lam03_mean, lam03_lower, lam03_upper = mean_ci(lam03_arr)\n",
    "\n",
    "    return {\n",
    "        \"lam01\": (lam01_mean, lam01_lower, lam01_upper),\n",
    "        \"lam02\": (lam02_mean, lam02_lower, lam02_upper),\n",
    "        \"lam03\": (lam03_mean, lam03_lower, lam03_upper)\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
