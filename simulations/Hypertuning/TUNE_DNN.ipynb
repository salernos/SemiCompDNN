{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93901ace-e45a-46e0-8662-85eaa68d8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import psi, gammaln\n",
    "from scipy.special import digamma\n",
    "import pyreadr\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c2f28b-21f3-429d-9962-690118e489ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1_fun_stable(h1, d1, y1, gamma):\n",
    "    h1 = h1.view(-1)\n",
    "    d1 = d1.view(-1)\n",
    "    y1 = y1.view(-1)\n",
    "    gamma = gamma.view(-1)\n",
    "\n",
    "    risk_matrix = (y1.view(-1, 1) <= y1.view(1, -1)).float()\n",
    "\n",
    "    h1_stable = h1 - torch.max(h1)\n",
    "    exp_h1 = gamma * torch.exp(h1_stable)\n",
    "\n",
    "    denom = torch.matmul(risk_matrix, exp_h1) + 1e-8\n",
    "    log_risk = torch.log(denom) + torch.max(h1)\n",
    "\n",
    "    loss_vector = -d1 * (h1 - log_risk)\n",
    "\n",
    "    return loss_vector.sum() / (d1.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3985f-9325-41a5-b464-1a098eb5d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss2_fun_stable(h2, d1, d2, y1, gamma):\n",
    "    h2 = h2.view(-1)\n",
    "    d1 = d1.view(-1)\n",
    "    d2 = d2.view(-1)\n",
    "    y1 = y1.view(-1)\n",
    "    gamma = gamma.view(-1)\n",
    "\n",
    "    event_mask = ((d1 == 0) & (d2 == 1)).float()\n",
    "    risk_matrix = (y1.view(-1, 1) <= y1.view(1, -1)).float()\n",
    "\n",
    "    h2_stable = h2 - torch.max(h2)\n",
    "    exp_h2 = gamma * torch.exp(h2_stable)\n",
    "\n",
    "    denom = torch.matmul(risk_matrix, exp_h2) + 1e-8\n",
    "    log_risk = torch.log(denom) + torch.max(h2)\n",
    "\n",
    "    loss_vector = -event_mask * (h2 - log_risk)\n",
    "    return loss_vector.sum() / (event_mask.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb94e2a-ba3b-4f50-9fd7-c57222644a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss3_fun_stable(h3, d1, d2, y1, y2, gamma):\n",
    "    h3 = h3.view(-1)\n",
    "    d1 = d1.view(-1)\n",
    "    d2 = d2.view(-1)\n",
    "    y1 = y1.view(-1)\n",
    "    y2 = y2.view(-1)\n",
    "    gamma = gamma.view(-1)\n",
    "\n",
    "    event_mask = ((d1 == 1) & (d2 == 1)).float()\n",
    "\n",
    "    risk_matrix = ((y1.view(1, -1) < y2.view(-1, 1)) &\n",
    "                   (y2.view(1, -1) >= y2.view(-1, 1))).float()\n",
    "\n",
    "    h3_stable = h3 - torch.max(h3)\n",
    "    exp_h3 = gamma * d1 * torch.exp(h3_stable)\n",
    "\n",
    "    denom = torch.matmul(risk_matrix, exp_h3) + 1e-8\n",
    "    log_risk = torch.log(denom) + torch.max(h3)\n",
    "\n",
    "    loss_vector = -event_mask * (h3 - log_risk)\n",
    "\n",
    "    return loss_vector.sum() / (event_mask.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d8610-7de7-4f72-848e-3350bc5436ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dnn(formula, data, na_action=\"na.fail\", subset=None,\n",
    "            dim_layers=[128, 64, 16], lr=0.01, dr=0.1,\n",
    "            max_epochs=250, max_epochs_theta=100, max_epochs_n=5, verbose=True,\n",
    "            ll=1, tol=1e-6, theta0=0.5, lr_theta=0.01, batch_size=128):\n",
    "    # Assertions\n",
    "    if na_action not in [\"na.fail\", \"na.omit\"]:\n",
    "        raise ValueError('na_action should be either \"na.fail\" or \"na.omit\"')\n",
    "\n",
    "    # Pre-Process Data\n",
    "    true_h1 = data['h1'].values\n",
    "    true_h2 = data['h2'].values\n",
    "    true_h3 = data['h3'].values\n",
    "    \n",
    "    # Outcomes\n",
    "    y1 = data['Y1'].values\n",
    "    d1 = data['D1'].values\n",
    "    y2 = data['Y2'].values\n",
    "    d2 = data['D2'].values\n",
    "\n",
    "    # Unique Failure Times by Transition\n",
    "    t1_obs = y1[d1 == 1]\n",
    "    t2_obs = y2[(d1 == 0) & (d2 == 1)]\n",
    "    t3_obs = y2[(d1 == 1) & (d2 == 1)]\n",
    "    t1 = np.unique(np.sort(t1_obs))\n",
    "    t2 = np.unique(np.sort(t2_obs))\n",
    "    t3 = np.unique(np.sort(t3_obs))\n",
    "    tol_val = tol\n",
    "\n",
    "    # Features\n",
    "    X1_mat = data[form['X']].values\n",
    "    X2_mat = data[form['X']].values\n",
    "    X3_mat = data[form['X']].values\n",
    "    n = X1_mat.shape[0]\n",
    "\n",
    "    # Initialize Parameters\n",
    "    theta = torch.tensor(theta0, requires_grad=True, dtype=torch.float32)\n",
    "    optimizer_theta = optim.Adam([theta], lr=lr_theta)\n",
    "\n",
    "    # Baseline Hazards\n",
    "    d1_j = np.array([np.sum(d1 * (y1 == t)) for t in t1])\n",
    "    n1_j = np.array([np.sum(y1 >= t) for t in t1])\n",
    "    lam01 = d1_j / n1_j\n",
    "\n",
    "    d2_j = np.array([np.sum((1 - d1) * d2 * (y2 == t)) for t in t2])\n",
    "    n2_j = np.array([np.sum((y2 >= t) & (y1 >= t)) for t in t2])\n",
    "    lam02 = d2_j / n2_j\n",
    "\n",
    "    d3_j = np.array([np.sum(d1 * d2 * (y2 == t)) for t in t3])\n",
    "    n3_j = np.array([np.sum(y2[d1==1] >= t) for t in t3])\n",
    "    lam03 = d3_j / n3_j\n",
    "\n",
    "    # Neural Network Sub-Architectures\n",
    "    input_dim = X1_mat.shape[1]\n",
    "    def build_model(input_dim, dim_layers, dropout):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_layers[0], bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_layers[0], dim_layers[1], bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_layers[1], dim_layers[2], bias=False),\n",
    "            nn.Linear(dim_layers[2], 1, bias=False)\n",
    "        )\n",
    "    h1_model = build_model(input_dim, dim_layers, dr)\n",
    "    h2_model = build_model(input_dim, dim_layers, dr)\n",
    "    h3_model = build_model(input_dim, dim_layers, dr)\n",
    "    optimizer_h1 = optim.Adam(h1_model.parameters(), lr=lr)\n",
    "    optimizer_h2 = optim.Adam(h2_model.parameters(), lr=lr)\n",
    "    optimizer_h3 = optim.Adam(h3_model.parameters(), lr=lr)\n",
    "\n",
    "    # Initial Risk Function Values\n",
    "    h1 = h1_model(torch.tensor(X1_mat, dtype=torch.float32))\n",
    "    h2 = h2_model(torch.tensor(X2_mat, dtype=torch.float32))\n",
    "    h3 = h3_model(torch.tensor(X3_mat, dtype=torch.float32))\n",
    "\n",
    "    # Internal Helper Functions for Baseline Hazards\n",
    "    def lam01_fun(t, gamma_val, h1_vals):\n",
    "        numer = np.sum(d1 * (y1 == t))\n",
    "        h1_np = h1_vals.detach().cpu().numpy().flatten()\n",
    "        denom = np.sum(gamma_val * ((y1 >= t).astype(float)) * np.exp(h1_np))\n",
    "        return numer/denom if numer > 0 else 0\n",
    "\n",
    "    def lam02_fun(t, gamma_val, h2_vals):\n",
    "        numer = np.sum((1 - d1) * d2 * (y1 == t))\n",
    "        h2_np = h2_vals.detach().cpu().numpy().flatten()\n",
    "        denom = np.sum(gamma_val * (((y2 >= t) & (y1 >= t)).astype(float)) * np.exp(h2_np))\n",
    "        return numer/denom if numer > 0 else 0\n",
    "\n",
    "    def lam03_fun(t, gamma_val, h3_vals):\n",
    "        numer = np.sum(d1 * d2 * (y2 == t))\n",
    "        h3_np = h3_vals.detach().cpu().numpy().flatten()\n",
    "        denom = np.sum(gamma_val * d1 * ((y2 >= t).astype(float) - (y1 >= t).astype(float)) * np.exp(h3_np))\n",
    "        return numer/denom if numer > 0 else 0\n",
    "    \n",
    "    def Lam01_fun(t, lam01):\n",
    "        return np.sum(lam01[t1 - t < tol_val])\n",
    "    def Lam02_fun(t, lam02):\n",
    "        return np.sum(lam02[t2 - t < tol_val])\n",
    "    def Lam03_fun(t, lam03):\n",
    "        return np.sum(lam03[t3 - t < tol_val])\n",
    "\n",
    "    # Theta Contribution to Expected Log-Likelihood\n",
    "    def loss4(theta_tensor, gamma_tensor, log_gamma_tensor):\n",
    "        loss = -1/theta_tensor * torch.log(theta_tensor) + (1/theta_tensor - 1)*log_gamma_tensor \\\n",
    "               - 1/theta_tensor * gamma_tensor - torch.lgamma(1/theta_tensor)\n",
    "        return -torch.sum(loss)\n",
    "\n",
    "    # Dataset Class for Dataloader\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, inputs, outputs):\n",
    "            self.inputs = inputs\n",
    "            self.outputs = outputs\n",
    "        def __len__(self):\n",
    "            return self.inputs.shape[0]\n",
    "        def __getitem__(self, idx):\n",
    "            return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "    # Neural EM-Algorithm Loop\n",
    "\n",
    "    # Initialize loss tracking (for diagnostic purposes)\n",
    "    loss_h1_epoch = [np.inf]*(max_epochs+1)\n",
    "    loss_h2_epoch = [np.inf]*(max_epochs+1)\n",
    "    loss_h3_epoch = [np.inf]*(max_epochs+1)\n",
    "    loss_theta_epoch = [np.inf]*(max_epochs+1)\n",
    "    #loss_marginal_epoch = [np.inf]*(max_epochs+1)\n",
    "    \n",
    "    loss_theta_inc = 0\n",
    "    loss_h1_inc = 0\n",
    "    loss_h2_inc = 0\n",
    "    loss_h3_inc = 0\n",
    "    gamma_val = np.ones_like(y1)\n",
    "    log_gamma_val = np.zeros_like(y1)\n",
    "    diff_EM = 100\n",
    "    epoch = 2\n",
    "\n",
    "    while epoch < max_epochs and diff_EM > tol_val:\n",
    "        # E-STEP\n",
    "        theta_val = theta.item()\n",
    "        a = 1/theta_val + d1 + d2\n",
    "        # Update risk functions\n",
    "        h1_all = h1_model(torch.tensor(X1_mat, dtype=torch.float32)).detach()\n",
    "        h2_all = h2_model(torch.tensor(X2_mat, dtype=torch.float32)).detach()\n",
    "        h3_all = h3_model(torch.tensor(X3_mat, dtype=torch.float32)).detach()\n",
    "        # Compute cumulative hazards for each subject\n",
    "        Lam01 = np.array([Lam01_fun(t, lam01) for t in y1])\n",
    "        Lam02 = np.array([Lam02_fun(t, lam02) for t in y1])\n",
    "        Lam03_y2 = np.array([Lam03_fun(t, lam03) for t in y2])\n",
    "        Lam03_y1 = np.array([Lam03_fun(t, lam03) for t in y1])\n",
    "        h1_np = h1_all.numpy().flatten()\n",
    "        h2_np = h2_all.numpy().flatten()\n",
    "        h3_np = h3_all.numpy().flatten()\n",
    "        b = 1/theta_val + Lam01 * np.exp(h1_np) + Lam02 * np.exp(h2_np) + d1 * (Lam03_y2 - Lam03_y1) * np.exp(h3_np)\n",
    "        gamma_val = a / b\n",
    "        log_gamma_val = digamma(a) - np.log(b)\n",
    "\n",
    "        print(\"mean:\", np.mean(gamma_val))\n",
    "        print(\"var:\", np.var(gamma_val))\n",
    "\n",
    "        # M-STEP\n",
    "        lam01_new = np.array([lam01_fun(t, gamma_val, h1_all) for t in t1])\n",
    "        lam02_new = np.array([lam02_fun(t, gamma_val, h2_all) for t in t2])\n",
    "        lam03_new = np.array([lam03_fun(t, gamma_val, h3_all) for t in t3])\n",
    "        diff_EM = max(np.max(np.abs((lam01 - lam01_new) / (lam01 + 1e-8))),\n",
    "                    np.max(np.abs((lam02 - lam02_new) / (lam02 + 1e-8))),\n",
    "                    np.max(np.abs((lam03 - lam03_new) / (lam03 + 1e-8))))\n",
    "        lam01 = lam01_new\n",
    "        lam02 = lam02_new\n",
    "        lam03 = lam03_new\n",
    "\n",
    "        # N-STEP\n",
    "        # Update Frailty Variance\n",
    "        loss_theta_inc_inner = 0\n",
    "        prev_loss_theta_inner = float('inf')\n",
    "        \n",
    "        if loss_theta_inc < 2:\n",
    "            for epoch_theta in range(max_epochs_theta):\n",
    "                optimizer_theta.zero_grad()\n",
    "                loss_theta = loss4(theta,\n",
    "                                     torch.tensor(gamma_val, dtype=torch.float32),\n",
    "                                     torch.tensor(log_gamma_val, dtype=torch.float32))\n",
    "                loss_theta.backward(retain_graph=True)\n",
    "                optimizer_theta.step()\n",
    "                current_loss_theta = loss_theta.item()\n",
    "                if abs(current_loss_theta - prev_loss_theta_inner) <= 1e-4:\n",
    "                    break\n",
    "                \n",
    "                elif current_loss_theta > prev_loss_theta_inner:\n",
    "                    loss_theta_inc_inner += 1\n",
    "                    if loss_theta_inc_inner > 2:\n",
    "                        break\n",
    "                if verbose:\n",
    "                    print(f\"Theta Epoch {epoch_theta}, Loss: {current_loss_theta}, Theta: {theta.item()}\")\n",
    "                \n",
    "                prev_loss_theta_inner = current_loss_theta\n",
    "            \n",
    "            if current_loss_theta > loss_theta_epoch[epoch-1]:\n",
    "                loss_theta_inc += 1\n",
    "            else:\n",
    "                loss_theta_inc = 0\n",
    "            \n",
    "            loss_theta_epoch[epoch] = current_loss_theta\n",
    "\n",
    "        # Update Neural Network Parameters\n",
    "        outputs1 = np.column_stack((y1, d1, y2, d2, gamma_val, true_h1))\n",
    "        dataset1 = CustomDataset(torch.tensor(X1_mat, dtype=torch.float32),\n",
    "                                 torch.tensor(outputs1, dtype=torch.float32))\n",
    "        dl1 = DataLoader(dataset1, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        outputs2 = np.column_stack((y1, d1, y2, d2, gamma_val, true_h2))\n",
    "        dataset2 = CustomDataset(torch.tensor(X2_mat, dtype=torch.float32),\n",
    "                                 torch.tensor(outputs2, dtype=torch.float32))\n",
    "        dl2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        outputs3 = np.column_stack((y1, d1, y2, d2, gamma_val, true_h3))\n",
    "        dataset3 = CustomDataset(torch.tensor(X3_mat, dtype=torch.float32),\n",
    "                                 torch.tensor(outputs3, dtype=torch.float32))\n",
    "        dl3 = DataLoader(dataset3, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        loss1_val, loss2_val, loss3_val = float('inf'), float('inf'), float('inf')\n",
    "        \n",
    "\n",
    "        if loss_h1_inc < 2:\n",
    "            \n",
    "            loss_h1_epoch_n = 0\n",
    "\n",
    "            num_batches1 = 0\n",
    "\n",
    "            # Transition 1\n",
    "            for batch_X1, batch_out in dl1:\n",
    "                num_batches1 += 1\n",
    "                batch_y1 = batch_out[:, 0]\n",
    "                batch_d1 = batch_out[:, 1]\n",
    "                batch_y2 = batch_out[:, 2]\n",
    "                batch_d2 = batch_out[:, 3]\n",
    "                batch_gamma = batch_out[:, 4]\n",
    "                batch_true_h1 = batch_out[:, 5]\n",
    "                h1_batch = h1_model(batch_X1)\n",
    "                event_idx = (batch_d1 == 1).nonzero(as_tuple=True)[0]\n",
    "                loss_h1 = 0.0\n",
    "                if len(event_idx) > 0:\n",
    "                    loss_h1 = loss1_fun_stable(h1_batch, batch_d1, batch_y1, batch_gamma)\n",
    "                    optimizer_h1.zero_grad()\n",
    "                    loss_h1.backward(retain_graph=True)\n",
    "                    optimizer_h1.step()\n",
    "\n",
    "                loss_h1_epoch_n += loss_h1.item()\n",
    "\n",
    "            if num_batches1 > 0:\n",
    "                loss1_val = loss_h1_epoch_n / num_batches1\n",
    "            \n",
    "            if loss1_val > loss_h1_epoch[epoch-1]:\n",
    "                loss_h1_inc += 1\n",
    "            else:\n",
    "                loss_h1_inc = 0\n",
    "\n",
    "            loss_h1_epoch[epoch] = loss1_val\n",
    "            h1 = h1_model(torch.tensor(X1_mat, dtype=torch.float32))\n",
    "        \n",
    "        if loss_h2_inc < 2:\n",
    "            \n",
    "            loss_h2_epoch_n = 0\n",
    "            \n",
    "            num_batches2 = 0\n",
    "            \n",
    "            # Transition 2\n",
    "            for batch_X2, batch_out in dl2:\n",
    "                num_batches2 += 1\n",
    "                batch_y1 = batch_out[:, 0]\n",
    "                batch_d1 = batch_out[:, 1]\n",
    "                batch_y2 = batch_out[:, 2]\n",
    "                batch_d2 = batch_out[:, 3]\n",
    "                batch_gamma = batch_out[:, 4]\n",
    "                batch_true_h2 = batch_out[:, 5]\n",
    "                h2_batch = h2_model(batch_X2)\n",
    "                event_idx = ((batch_d1 == 0) & (batch_d2 == 1)).nonzero(as_tuple=True)[0]\n",
    "                loss_h2 = 0.0\n",
    "                if len(event_idx) > 0:\n",
    "                    loss_h2 = loss2_fun_stable(h2_batch, batch_d1, batch_d2, batch_y1, batch_gamma)\n",
    "                    optimizer_h2.zero_grad()\n",
    "                    loss_h2.backward(retain_graph=True)\n",
    "                    optimizer_h2.step()\n",
    "                loss_h2_epoch_n += loss_h2.item()\n",
    "                \n",
    "            if num_batches2 > 0:\n",
    "                loss2_val = loss_h2_epoch_n / num_batches2\n",
    "                \n",
    "            \n",
    "            if loss2_val > loss_h2_epoch[epoch-1]:\n",
    "                loss_h2_inc += 1\n",
    "            else:\n",
    "                loss_h2_inc = 0\n",
    "            \n",
    "            loss_h2_epoch[epoch] = loss2_val\n",
    "            h2 = h2_model(torch.tensor(X2_mat, dtype=torch.float32))\n",
    "            \n",
    "        if loss_h3_inc < 2:\n",
    "            \n",
    "            loss_h3_epoch_n = 0\n",
    "            \n",
    "            num_batches3 = 0\n",
    "            \n",
    "            # Transition 3\n",
    "            for batch_X3, batch_out in dl3:\n",
    "                num_batches3 += 1\n",
    "                batch_y1 = batch_out[:, 0]\n",
    "                batch_d1 = batch_out[:, 1]\n",
    "                batch_y2 = batch_out[:, 2]\n",
    "                batch_d2 = batch_out[:, 3]\n",
    "                batch_gamma = batch_out[:, 4]\n",
    "                batch_true_h3 = batch_out[:, 5]\n",
    "                h3_batch = h3_model(batch_X3)\n",
    "                event_idx = ((batch_d1 == 1) & (batch_d2 == 1)).nonzero(as_tuple=True)[0]\n",
    "                loss_h3 = 0.0\n",
    "                if len(event_idx) > 0:\n",
    "                    loss_h3 = loss3_fun_stable(h3_batch, batch_d1, batch_d2, batch_y1, batch_y2, batch_gamma)\n",
    "                    optimizer_h3.zero_grad()\n",
    "                    loss_h3.backward(retain_graph=True)\n",
    "                    optimizer_h3.step()\n",
    "                loss_h3_epoch_n += loss_h3.item()\n",
    "            \n",
    "            if num_batches3 > 0:\n",
    "                loss3_val = loss_h3_epoch_n / num_batches3\n",
    "                \n",
    "            if loss3_val > loss_h3_epoch[epoch-1]:\n",
    "                loss_h3_inc += 1\n",
    "            else:\n",
    "                loss_h3_inc = 0\n",
    "\n",
    "            loss_h3_epoch[epoch] = loss3_val\n",
    "            h3 = h3_model(torch.tensor(X3_mat, dtype=torch.float32))\n",
    "\n",
    "        # Plot Baseline Hazards\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(t1, np.cumsum(lam01), marker='o')\n",
    "        plt.title('Baseline Hazards 1')\n",
    "        plt.xlabel('Failure Time')\n",
    "        plt.ylabel('Cumulative Baseline Hazards')\n",
    "        plt.axline((0, 0), slope=2, linestyle='--')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(t2, np.cumsum(lam02), marker='o')\n",
    "        plt.title('Baseline Hazards 2')\n",
    "        plt.xlabel('Failure Time')\n",
    "        plt.ylabel('Cumulative Baseline Hazards')\n",
    "        plt.axline((0, 0), slope=3, linestyle='--')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(t3, np.cumsum(lam03), marker='o')\n",
    "        plt.title('Baseline Hazards 3')\n",
    "        plt.xlabel('Failure Time')\n",
    "        plt.ylabel('Cumulative Baseline Hazards')\n",
    "        plt.axline((0, 0), slope=2, linestyle='--')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss1: {loss1_val}, loss2: {loss2_val}, loss3: {loss3_val}\")\n",
    "        epoch += 1\n",
    "        diff_EM = 100  # Reset diff_EM for next outer iteration\n",
    "\n",
    "        epochs = range(1, len(loss_h1_epoch))\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.plot(epochs, loss_h1_epoch[1:], label=\"Loss h1\")\n",
    "        plt.plot(epochs, loss_h2_epoch[1:], label=\"Loss h2\")\n",
    "        plt.plot(epochs, loss_h3_epoch[1:], label=\"Loss h3\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Curves\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if loss_h1_inc >= 2 and loss_h2_inc >= 2 and loss_h3_inc >=2:\n",
    "            \n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"theta\": theta.item(),\n",
    "        \"lam01\": lam01,\n",
    "        \"lam02\": lam02,\n",
    "        \"lam03\": lam03,\n",
    "        \"h1\": h1,\n",
    "        \"h2\": h2,\n",
    "        \"h3\": h3,\n",
    "        \"gamma\": gamma_val,\n",
    "        \"loss_theta\": current_loss_theta,\n",
    "        \"loss_h1\":loss1_val,\n",
    "        \"loss_h2\":loss2_val,\n",
    "        \"loss_h3\":loss3_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912a85c-fdb6-407f-85bc-734d50a17ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(sim, dat, form):\n",
    "    dat_sim = pd.DataFrame(dat[:, :, sim])\n",
    "    dat_sim.columns = [\"Y1\", \"D1\", \"Y2\", \"D2\"] + [f\"X{i}\" for i in range(1, 13)] + \\\n",
    "                      [\"h1\", \"h2\", \"h3\", \"hc\", \"gamma\", \"Ti1\", \"Ti2\", \"Ti3\", \"ctime\",\n",
    "                       \"cind\", \"setting\", \"n\", \"theta\", \"risk\", \"cens\"]\n",
    "    \n",
    "    fit_00 = fit_dnn(form, dat_sim,\n",
    "                     na_action=\"na.fail\", subset=None, dim_layers=[256, 128, 16],\n",
    "                     lr=0.001, dr=0.3,\n",
    "                     max_epochs=50, max_epochs_theta=10, max_epochs_n=5,\n",
    "                     batch_size=500, verbose=True, ll=1, tol=1e-6,\n",
    "                     theta0=4, lr_theta=0.01)\n",
    "    \n",
    "    result = {\n",
    "    \"theta\": fit_00[\"theta\"],\n",
    "    \"lam01\": fit_00[\"lam01\"],\n",
    "    \"lam02\": fit_00[\"lam02\"],\n",
    "    \"lam03\": fit_00[\"lam03\"],\n",
    "    \"h1\": fit_00[\"h1\"].detach().numpy(),\n",
    "    \"h2\": fit_00[\"h2\"].detach().numpy(),\n",
    "    \"h3\": fit_00[\"h3\"].detach().numpy(),\n",
    "    \"gamma\": fit_00[\"gamma\"],\n",
    "    \"loss_h1\":fit_00[\"loss_h1\"],\n",
    "    \"loss_h2\":fit_00[\"loss_h2\"],\n",
    "    \"loss_h3\":fit_00[\"loss_h3\"]\n",
    "        \n",
    "}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea1601-a3ff-4c43-a9df-6e61d93a1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [1024, 512, 256, 128, 64, 32, 16]\n",
    "candidates = [list(combo) for combo in itertools.product(layer_sizes, repeat=3)\n",
    "              if combo[0] > combo[1] > combo[2]]\n",
    "\n",
    "print(f\"Generated {len(candidates)} candidates.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for dims in candidates:\n",
    "    print(f\"Testing {dims}...\")\n",
    "\n",
    "    run_simulation(sim, dat, form)\n",
    "    loss_theta = result[\"loss_theta\"]\n",
    "    loss_h1 = result[\"loss_h1\"]\n",
    "    loss_h2 = result[\"loss_h2\"]\n",
    "    loss_h3 = result[\"loss_h3\"]\n",
    "    total_loss = loss_theta + loss_h1 + loss_h2 + loss_h3\n",
    "\n",
    "    print(f\"Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"dim_layers\": dims,\n",
    "        \"loss_theta\": loss_theta,\n",
    "        \"loss_h1\": loss_h1,\n",
    "        \"loss_h2\": loss_h2,\n",
    "        \"loss_h3\": loss_h3,\n",
    "        \"total_loss\": total_loss\n",
    "    })\n",
    "\n",
    "# Sort by Best\n",
    "results_sorted = sorted(results, key=lambda x: x[\"total_loss\"])\n",
    "best_result = results_sorted[0]\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"Hidden Layers: {best_result['dim_layers']}\")\n",
    "print(f\"Total Loss: {best_result['total_loss']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
